{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 2.00/2.00 [00:00<?, ?B/s]\n",
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lowed001\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.53k/1.53k [00:00<?, ?B/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 229k/229k [00:00<00:00, 382kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 112kB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mindobenchmark/indobert-base-p1\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m AutoModelForTokenClassification\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\n\u001b[0;32m      9\u001b[0m \u001b[39m# Teks yang akan dianalisis\u001b[39;00m\n\u001b[0;32m     10\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPulau Bali terletak di Indonesia. Presiden Joko Widodo akan berkunjung ke Bali besok.\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1124\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1124\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1103\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[39m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[39m.\u001b[39mformat(name))\n\u001b[0;32m   1105\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Muat model bahasa dan model NER untuk bahasa Indonesia\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Teks yang akan dianalisis\n",
    "text = \"Pulau Bali terletak di Indonesia. Presiden Joko Widodo akan berkunjung ke Bali besok.\"\n",
    "\n",
    "# Tokenisasi teks\n",
    "tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))\n",
    "\n",
    "# Inferensi dengan model NER\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# Ambil label entitas\n",
    "predicted_labels = torch.argmax(outputs.logits, dim=2)[0].tolist()\n",
    "\n",
    "# Daftar label NER yang sesuai dengan model yang digunakan\n",
    "label_list = [\"O\", \"B-LOC\", \"I-LOC\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-MISC\", \"I-MISC\"]\n",
    "\n",
    "# Cetak hasil entitas yang ditemukan\n",
    "entities = []\n",
    "current_entity = {\"text\": \"\", \"label\": \"\"}\n",
    "for token, label_id in zip(tokens, predicted_labels):\n",
    "    label = label_list[label_id]\n",
    "    if label.startswith(\"B-\"):\n",
    "        if current_entity[\"text\"]:\n",
    "            entities.append(current_entity)\n",
    "        current_entity = {\"text\": token, \"label\": label[2:]}\n",
    "    elif label.startswith(\"I-\"):\n",
    "        current_entity[\"text\"] += \" \" + token\n",
    "\n",
    "# Cek entitas terakhir\n",
    "if current_entity[\"text\"]:\n",
    "    entities.append(current_entity)\n",
    "\n",
    "# Cetak hasil entitas yang ditemukan beserta labelnya\n",
    "for entity in entities:\n",
    "    print(f\"Entitas: {entity['text']}, Label: {entity['label']}\")\n",
    "\n",
    "# Contoh output:\n",
    "# Entitas: Bali, Label: LOC\n",
    "# Entitas: Indonesia, Label: LOC\n",
    "# Entitas: Joko Widodo, Label: PER\n",
    "# Entitas: Bali, Label: LOC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading tf_model.h5: 100%|██████████| 656M/656M [02:50<00:00, 3.85MB/s] \n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teks: Ini adalah produk yang sangat bagus. Saya sangat puas.\n",
      "Sentimen: Positif\n",
      "\n",
      "Teks: Produk ini sangat buruk. Saya tidak puas sama sekali.\n",
      "Sentimen: Positif\n",
      "\n",
      "Teks: Produknya biasa saja, tidak terlalu bagus dan tidak terlalu buruk.\n",
      "Sentimen: Positif\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "# Muat model bahasa dan model klasifikasi sentimen untuk bahasa Indonesia\n",
    "model_name = \"indobenchmark/indobert-base-p1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Teks yang akan dianalisis\n",
    "texts = [\n",
    "    \"Ini adalah produk yang sangat bagus. Saya sangat puas.\",\n",
    "    \"Produk ini sangat buruk. Saya tidak puas sama sekali.\",\n",
    "    \"Produknya biasa saja, tidak terlalu bagus dan tidak terlalu buruk.\"\n",
    "]\n",
    "\n",
    "# Tokenisasi dan klasifikasi sentimen\n",
    "sentiments = []\n",
    "for text in texts:\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "    predicted_class = tf.argmax(outputs.logits, axis=1).numpy()[0]\n",
    "    if predicted_class == 0:\n",
    "        \\sentiment = \"Negatif\"\n",
    "    elif predicted_class == 1:\n",
    "        sentiment = \"Netral\"\n",
    "    else:\n",
    "        sentiment = \"Positif\"\n",
    "    sentiments.append(sentiment)\n",
    "\n",
    "# Cetak hasil sentimen\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Teks: {text}\")\n",
    "    print(f\"Sentimen: {sentiments[i]}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4490, 0.9854, 0.5977],\n",
      "        [0.3534, 0.8899, 0.1698],\n",
      "        [0.2985, 0.7845, 0.9545],\n",
      "        [0.0732, 0.1769, 0.6122],\n",
      "        [0.5387, 0.2050, 0.5387]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
